---
title: "第十一周：预测分类结果：Logistic 回归（一）"
---

## 1. 从线性回归到分类预测

到目前为止，我们学习的回归模型（SLR, MLR）都是用于预测**连续型**因变量 Y。但很多时候，我们感兴趣的预测目标是**分类变量**，特别是**二元分类 (Binary Classification)** 变量，其结果只有两种可能（例如：是/否、成功/失败、购买/不购买、患病/未患病）。

-   **线性回归的局限性:**
    -   如果直接用线性回归预测一个 0/1 的二元变量，预测值 $\hat{y}$ 可能会超出 \[0, 1\] 的合理范围。
    -   误差项不满足正态性和等方差性假设。
    -   因变量和自变量之间的关系通常不是线性的，而是 S 形的。
-   **Logistic 回归 (Logistic Regression):**
    -   一种广泛用于处理**二元或多元分类**问题的**广义线性模型 (Generalized Linear Model, GLM)**。
    -   它不直接预测类别 (0 或 1)，而是预测属于某个类别（通常是 "成功" 或 "事件发生"，编码为 1）的**概率 (Probability)** $P(Y=1|X)$。
    -   然后可以设定一个阈值（如 0.5），将预测概率转换为类别预测。

::: {.callout-note title="本周目标"}
-   理解为何线性回归不适用于分类结果。
-   掌握 Logistic 回归的基本原理：Sigmoid 函数和 Logit 变换。
-   理解 Logistic 回归的模型形式。
-   **重点掌握** Logistic 回归系数的解释，特别是**优势比 (Odds Ratio, OR)**。
-   能够使用 R 的 `glm()` 函数拟合二元 Logistic 回归模型。
-   **了解综合实践项目的要求、选题方向、时间安排和评分标准。**
:::

## 2. Logistic 回归原理

Logistic 回归通过两个关键步骤将线性预测值与概率联系起来：

1.  **线性预测器 (Linear Predictor):** 与线性回归类似，计算自变量的线性组合： $$ z = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k $$ 这个 $z$ 的取值范围是 $(-\infty, +\infty)$。

2.  **连接函数 (Link Function) 的逆转换 / Sigmoid 函数:** 为了将 $z$ 映射到 (0, 1) 的概率区间，Logistic 回归使用了 **Sigmoid 函数** (也称为 Logistic 函数)： $$ P(Y=1|X) = p = \frac{e^z}{1 + e^z} = \frac{1}{1 + e^{-z}} $$

    -   Sigmoid 函数的图形呈 S 形，可以将任何实数 $z$ 转换为 (0, 1) 之间的值。
    -   当 $z \to +\infty$ 时，$p \to 1$。
    -   当 $z \to -\infty$ 时，$p \to 0$。
    -   当 $z = 0$ 时，$p = 0.5$。

```{r sigmoid_plot}
#| code-fold: true
library(tidyverse)
# 生成一系列 z 值
z <- seq(-10, 10, length.out = 100)

# 计算对应的概率 p
p <- 1 / (1 + exp(-z))

# 绘制 Sigmoid 函数图像
ggplot(data.frame(z = z, p = p), aes(x = z, y = p)) +
  geom_line(color = "blue") +
  labs(x = "z", y = "p(z)", title = "Sigmoid Function") +
  theme_minimal()
```



::: {.callout-tip title="什么是连接函数 (Link Function)？"}
连接函数（Link Function）是广义线性模型（GLM）中的一个重要概念。它的作用是将因变量的期望值（如概率 $p$）与自变量的线性组合（$z = \beta_0 + \beta_1 X_1 + ...$）联系起来。通过连接函数，可以将不同类型的因变量（如概率、计数等）与线性预测器相结合，实现灵活的建模。例如，Logistic 回归中用 Logit 连接函数将概率 $p$ 映射到 $(-\infty, +\infty)$ 的对数优势（log-odds）空间。
:::

::: {.callout-tip title="为什么会使用 Sigmoid 函数？"}
在 Logistic 回归中，我们需要将线性预测器 $z$（取值范围为 $(-\infty, +\infty)$）转换为概率 $p$（取值范围为 $(0, 1)$）。Sigmoid 函数（Logistic 函数）正好具备这种特性：$p = \frac{1}{1 + e^{-z}}$。它不仅保证输出在 $(0, 1)$ 区间，还能很好地描述概率随自变量变化的 S 形趋势。因此，Sigmoid 函数成为二元 Logistic 回归的标准选择。
:::

::: {.callout-tip title="一定要使用 Sigmoid 函数吗？"}
不一定。虽然 Logistic 回归通常使用 Sigmoid（Logit 连接函数），但在广义线性模型（GLM）框架下，连接函数是可以更换的。例如，Probit 回归使用正态分布的累积分布函数（CDF）作为连接函数，Cloglog 回归使用补对数-对数（complementary log-log）连接函数。不同的连接函数适用于不同的数据分布和建模需求。实际中，Sigmoid 最常用，但不是唯一选择。
:::


3.  **Logit 变换 (Logit Transformation) / 连接函数:** 反过来，如果我们想将概率 $p$ 转换回线性预测器 $z$，需要用到 Sigmoid 函数的反函数，即 **Logit 变换**：

    -   **优势 (Odds):** 事件发生的概率与不发生的概率之比。$Odds = \frac{p}{1-p}$。取值范围是 $(0, +\infty)$。
    -   **对数优势 (Log-odds) / Logit:** 对优势取自然对数。$Logit(p) = \log(\frac{p}{1-p})$。取值范围是 $(-\infty, +\infty)$。
    -   在 Logistic 回归中，**Logit(p) 被假定为自变量的线性组合**: $$ \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k $$ 这里的 Logit 函数就是 Logistic 回归的**连接函数**，它将非线性的概率关系转换为了线性的对数优势关系。

::: {.callout-note title="优势 (Odds) 的实际应用场景"}
优势（Odds）及其对数优势（log-odds）在实际中有广泛应用，尤其是在医学、流行病学、社会科学等领域：

- **医学/流行病学**：用于比较暴露组与非暴露组某疾病发生的优势（如吸烟者与非吸烟者患肺癌的优势比）。
- **社会科学**：分析某因素（如教育水平、性别）对某事件（如就业、投票）的影响。
- **金融风控**：评估客户特征对违约概率的影响，优势比可量化某变量对违约风险的提升或降低。
- **市场营销**：衡量促销活动、广告等对购买行为发生的优势变化。
- **心理学/行为科学**：分析某干预措施对行为发生的优势提升。

优势（Odds）和优势比（Odds Ratio, OR）为解释二元结局变量（如“是/否”）与自变量关系提供了直观的量化工具，尤其适用于事件发生率较低的场景。
:::

::: {.callout-tip title="什么是相对风险（Relative Risk, RR）？"}
**相对风险（Relative Risk, RR）**，又称为风险比，是指暴露组事件发生的概率与非暴露组事件发生的概率之比。其计算公式为：

$$
RR = \frac{P(\text{事件}| \text{暴露})}{P(\text{事件}| \text{非暴露})}
$$

- 若 $RR = 1$，表示暴露与事件发生无关；
- 若 $RR > 1$，表示暴露会增加事件发生的风险；
- 若 $RR < 1$，表示暴露会降低事件发生的风险。

**举例：** 如果吸烟者患肺癌的概率为 0.10，非吸烟者为 0.02，则 $RR = 0.10 / 0.02 = 5$，即吸烟者患肺癌的风险是非吸烟者的 5 倍。

**实际解读时，很多情况下我们更关心的是相对风险（RR），因为它直接反映概率的倍数关系，更直观、更贴近实际意义。**
:::

::: {.callout-tip title="为什么优势比（OR）适用于事件发生概率较低的情况？"}
当事件发生概率较低（即罕见事件）时，优势比（OR）与相对风险（RR）非常接近，二者数值差异很小，因此用 OR 近似 RR 是合理且常见的做法。例如，在流行病学中研究罕见疾病时，OR 可以很好地反映暴露与疾病之间的风险关系。

**数学说明：**

假设有两组（暴露组和非暴露组），事件发生概率分别为 $p_1$ 和 $p_0$，则：

- **相对风险（RR）** 定义为：
  $$
  RR = \frac{p_1}{p_0}
  $$
- **优势比（OR）** 定义为：
  $$
  OR = \frac{p_1/(1-p_1)}{p_0/(1-p_0)} = \frac{p_1(1-p_0)}{p_0(1-p_1)}
  $$

当 $p_1$ 和 $p_0$ 都很小（即 $p_1, p_0 \ll 1$），$1-p_1 \approx 1$，$1-p_0 \approx 1$，此时：
$$
OR \approx \frac{p_1}{p_0} = RR
$$
因此在罕见事件（低发生率）情况下，OR 与 RR 数值非常接近。

但当事件发生概率较高时，OR 会高估实际的风险比（RR），解释时容易产生误导。具体来说，随着 $p_1$ 或 $p_0$ 增大，$1-p_1$ 和 $1-p_0$ 不再接近 1，OR 的分母变小，导致 OR 明显大于 RR。例如：

- 若 $p_0 = 0.5, p_1 = 0.75$，则 $RR = 0.75/0.5 = 1.5$，而 $OR = \frac{0.75/0.25}{0.5/0.5} = 3$，OR 明显大于 RR。

因此，优势比最适合用于低发生率（稀有事件）场景下的解释和比较；在高发生率场景下，需谨慎使用 OR，并建议同时报告 RR 以避免误导。
:::

::: {.callout-tip title="那为什么 Logistic 回归要用优势（Odds）而不是直接用相对风险（RR）？"}
实际上，在实际解读和政策建议时，我们**更关心相对风险（RR）**，因为它直接反映概率的变化，对非专业人士也更容易理解。

但在建模时，Logistic 回归选择用优势（Odds）和对数优势（log-odds）作为建模对象，主要原因有：

1. **概率的线性建模有限制**：概率 $p$ 的取值范围是 $(0,1)$，直接用线性回归建模会导致预测值超出 $[0,1]$ 区间，不合理。
2. **优势（Odds）和对数优势（log-odds）可以取任意实数**：对数优势 $log(\frac{p}{1-p})$ 的取值范围是 $(-\infty, +\infty)$，适合用线性模型来建模。
3. **Logistic 回归的数学结构**：Logistic 回归假定自变量的线性组合与对数优势（log-odds）成线性关系，即
   $$
   \log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + ... + \beta_k X_k
   $$
   这样可以用最大似然法方便地估计参数。
4. **优势比（OR）易于解释和计算**：对数优势的指数化（$e^{\beta_j}$）就是优势比，能直观地反映自变量变化对事件发生“优势”的倍数影响。

**总结：** Logistic 回归之所以用优势（Odds），主要是出于建模和数学推导的便利，而不是因为优势比本身比相对风险更有实际意义。实际解读时，尤其是向非专业人士汇报结果时，建议尽量补充相对风险（RR）的估算和解释。
:::

::: {.callout-warning title="实际应用建议"}
- **建模时**：Logistic 回归只能直接输出优势比（OR），因为其数学结构决定了只能建模 log-odds。
- **解读和报告时**：如果事件发生率不高，可以用 OR 近似 RR；如果事件发生率较高，建议用其他方法（如风险回归模型、G-computation、marginal effect 等）估算和报告相对风险（RR），以便更贴近实际意义。
:::


## 3. Logistic 回归模型形式

核心模型是： $$ \log\left(\frac{P(Y=1|X)}{1-P(Y=1|X)}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_k X_k $$

或者等价地写成概率形式： $$ P(Y=1|X) = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_k X_k)}} $$

-   模型参数 $\beta_0, \beta_1, ..., \beta_k$ 通常使用**最大似然估计 (Maximum Likelihood Estimation, MLE)** 来获得，而不是 OLS。

::: {.callout-note title="什么是最大似然估计 (Maximum Likelihood Estimation, MLE)？"}
**最大似然估计**是一种常用的参数估计方法，其核心思想是：在已知观测数据的前提下，选择一组参数，使得在这些参数下，观测到的数据出现的“可能性”（即**似然**）最大。

- **直观理解**：假如你知道数据是某种分布（比如二项分布、正态分布等）生成的，但不知道具体参数（如均值、方差等），最大似然估计就是找到一组参数，使得“在这些参数下，实际观测到的数据最有可能出现”。
- **数学表达**：设观测数据为 $y_1, y_2, ..., y_n$，参数为 $\theta$，则似然函数为 $L(\theta) = P(\text{数据}|\theta)$。最大似然估计就是求使 $L(\theta)$ 最大的 $\theta$。

:::

::: {.callout-tip title="Logistic 回归中最大似然估计的计算方法"}
在 Logistic 回归中，我们假设因变量 $Y_i$ 服从伯努利分布（0/1），其概率由自变量 $X$ 通过 logit 链接函数建模：

$$
P(Y_i = 1|X_i) = p_i = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_{i1} + ... + \beta_k X_{ik})}}
$$

- **似然函数**：所有观测的联合概率为
  $$
  L(\beta) = \prod_{i=1}^n p_i^{y_i} (1-p_i)^{1-y_i}
  $$
- **对数似然函数**（便于计算）：
  $$
  \ell(\beta) = \sum_{i=1}^n \left[ y_i \log(p_i) + (1-y_i)\log(1-p_i) \right]
  $$
- **最大似然估计过程**：
    1. 写出对数似然函数 $\ell(\beta)$。
    2. 对参数 $\beta$ 求偏导，令导数为 0，解方程组（通常无解析解）。
    3. 用数值优化算法（如牛顿-拉夫森法）迭代求解最优参数 $\hat{\beta}$。

- **R 中实现**：`glm()` 函数自动完成最大似然估计，无需手动推导和优化。

**总结**：Logistic 回归的系数估计本质上就是通过最大似然法，找到一组参数，使得在这些参数下，观测到的 0/1 结果最有可能出现。
:::


## 4. 系数解释：优势比 (Odds Ratio, OR) - 核心！

直接解释 Logistic 回归的系数 $\beta_j$ 比较困难，因为它表示自变量 $X_j$ 每增加一个单位，**对数优势 (Log-odds)** 的变化量。为了更直观地理解，我们通常解释**优势比 (Odds Ratio, OR)**。

-   **优势比 (OR):** 指自变量 $X_j$ **增加一个单位**时，事件发生的**优势 (Odds)** 变为原来的**多少倍**。 $$ OR_j = \frac{Odds(X_j+1)}{Odds(X_j)} = \frac{e^{\beta_0 + \beta_1 X_1 + ... + \beta_j(X_j+1) + ...}}{e^{\beta_0 + \beta_1 X_1 + ... + \beta_j X_j + ...}} = e^{\beta_j} $$
    -   **计算:** $OR_j = \exp(\beta_j)$。
-   **解读 OR:**
    -   $OR_j > 1$ ($\beta_j > 0$): $X_j$ 增加一个单位，事件发生的**优势增加** (变为原来的 $OR_j$ 倍)。$X_j$ 是风险因素/促进因素。
    -   $OR_j < 1$ ($\beta_j < 0$): $X_j$ 增加一个单位，事件发生的**优势减少** (变为原来的 $OR_j$ 倍)。$X_j$ 是保护因素。
    -   $OR_j = 1$ ($\beta_j = 0$): $X_j$ 变化对事件发生的优势**没有影响**。
-   **示例解释:**
    -   假设研究吸烟 (X=1 表示吸烟, X=0 表示不吸烟) 对患肺癌 (Y=1) 的影响，得到 $\hat{\beta}_{smoke} = 1.609$。
    -   计算 $OR_{smoke} = \exp(1.609) \approx 5.0$。
    -   **解释:** 吸烟者的患肺癌**优势**是**不吸烟者**的**5 倍** (在控制其他变量后)。
    -   假设研究年龄 (X，连续变量) 对购买某产品 (Y=1) 的影响，得到 $\hat{\beta}_{age} = -0.05$。
    -   计算 $OR_{age} = \exp(-0.05) \approx 0.95$。
    -   **解释:** 年龄**每增加 1 岁**，购买该产品的**优势**变为原来的**0.95 倍** (即降低了约 5%) (在控制其他变量后)。
-   **分类自变量的 OR:** 如果 $X_j$ 是一个分类变量（如用虚拟编码表示），$e^{\beta_j}$ 表示该类别相对于**参照类别**的优势比。

::: {.callout-warning title="OR vs RR"}
优势比 (OR) **不等于** 相对风险 (Relative Risk, RR = $P(Y=1|X_j+1) / P(Y=1|X_j)$)。只有当事件发生率很低时，OR 才近似等于 RR。解释时要用“优势”而非“风险”或“概率”。
:::

## 5. R 实现: `glm()` (Generalized Linear Model)

使用 `glm()` 函数拟合 Logistic 回归模型。

-   **关键参数:**

    -   `formula`: 与 `lm()` 类似，`Y ~ X1 + X2 + ...`。Y 应该是 0/1 编码或因子（R 会自动处理第一个水平为 0，第二个为 1）。
    -   `family`: 指定模型的分布族和连接函数。对于 Logistic 回归，使用 `family = binomial(link = "logit")` 或简写 `family = binomial`。
    -   `data`: 数据框。

-   **示例：** 使用 `ISLR` 包中的 `Default` 数据集，预测客户是否违约（default），基于信用卡余额（balance）和是否为学生（student）。

    ```{r}
    library(tidyverse)
    library(ISLR)
    library(broom)

    # 1. 数据准备：确保 default 是因子，student 也是因子
    data("Default")
    glimpse(Default)
    # 默认 default 已为因子，student 也是因子

    # 2. 拟合 Logistic 回归模型
    logistic_model <- glm(default ~ balance + student, data = Default, family = binomial)

    # 3. 查看模型摘要（系数、标准误、z值、P值等）
    tidy_logistic <- tidy(logistic_model)
    print(tidy_logistic)

    # 4. 计算系数的优势比（OR）及其置信区间
    # 4.1 计算 OR
    or_table <- tidy_logistic %>%
      mutate(OR = exp(estimate))
    print(or_table)

    # 4.2 计算置信区间（对数优势尺度）
    confint_logistic <- confint(logistic_model)
    print(confint_logistic)

    # 4.3 计算 OR 的置信区间
    or_ci <- exp(confint_logistic)
    print(or_ci)
    ```

-   **结果解读：**

    -   **系数表（tidy_logistic）** 包含每个变量的估计值（Estimate）、标准误（Std. Error）、z值（z value）和P值（Pr(>|z|)）。
    -   **优势比（OR）** 通过 `exp(estimate)` 计算，表示自变量每增加一个单位，违约的优势变化倍数。
    -   **置信区间** 通过 `confint()` 计算，`exp(confint())` 得到 OR 的置信区间。

    -   **如何解读：**
        -   查看 `or_table` 和 `or_ci` 的输出，结合 P 值判断变量是否显著影响违约概率。
        -   例如，若 balance 的 OR > 1 且 P 值显著，说明余额每增加 1 单位，违约的优势增加；若 student 的 OR < 1，说明学生身份对违约的优势有保护作用（或相反，视结果而定）。


## 6. 综合实践项目启动

本周，我们将正式启动课程的 **综合实践项目**！

-   **目标:** 应用本课程（主要是第一阶段，后续会补充第二阶段知识）所学的数据处理、可视化和统计推断/建模技能，选择一个你感兴趣的数据集和研究问题，完成一次相对完整的数据分析过程，并撰写报告或进行展示。
-   **选题方向:**
    -   可以使用课程提供的示例数据集。
    -   可以寻找公开数据集（如 Kaggle, UCI Machine Learning Repository, 政府公开数据平台等）。
    -   可以使用与你专业领域相关的数据（需确保可获取性）。
    -   **关键：** 选题范围要适中，问题要明确，数据应较为规整，或能够通过 `tidyverse` 工具进行清理。
-   **时间节点 (暂定，以老师最终通知为准):**
    -   **第 11-12 周:** 确定选题、研究问题、获取数据、初步数据探索 (EDA)。
    -   **第 13-14 周:** 数据清理、模型构建（选择合适的统计方法）、模型诊断与改进。
    -   **第 15 周:** 结果解释、报告撰写、展示准备、预演与反馈。
    -   **第 16 周:** 最终项目展示与答辩 / 报告提交。
-   **评分标准 (大致方向):**
    -   问题定义的清晰性与合理性。
    -   数据处理与准备的恰当性。
    -   探索性数据分析 (可视化) 的有效性。
    -   统计方法选择与应用的正确性。
    -   模型诊断与评估的完整性 (如适用)。
    -   结果解释的准确性与深入性。
    -   报告/展示的清晰度与专业性。

**本周任务:** 开始思考你的综合实践项目选题！寻找可能的数据集，明确你想要通过数据分析回答什么问题。下周我们将讨论选题并进行初步的数据探索。

## 7. 本周总结与预告

本周我们开启了分类预测的大门，学习了 Logistic 回归的基本原理，包括 Sigmoid 函数、Logit 变换以及最重要的优势比 (Odds Ratio) 解释。我们还掌握了使用 `glm()` 函数在 R 中拟合模型的方法。同时，综合实践项目正式启动，请大家积极思考选题。

**下周预告:** 拟合了 Logistic 回归模型，如何评估它的表现？下周我们将学习 Logistic 回归的**评估指标**，如混淆矩阵、准确率、精确率、召回率、F1 分数以及 ROC 曲线和 AUC 值，并探讨如何在不同模型间进行比较。