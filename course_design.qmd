---
title: "《统计学与R语言》课程设计：端到端数据智能应用开发周"
---

# 1. 项目概述 (Project Overview)

欢迎来到《统计学与R语言》课程的终极挑战——**“端到端数据智能应用开发周”**。本次课程设计旨在模拟一个真实世界的数据产品开发冲刺流程。你将不再是撰写一份静态的分析报告，而是以一个敏捷开发小组的身份，在为期五天的时间内，完成一个从数据获取、智能处理、统计分析到应用交付的完整闭环。

为确保项目的顺利进行，我们将**以精选且经过严格验证的Kaggle数据集作为项目的标准数据源**。这允许你将主要精力集中在数据分析、AI应用和系统构建上。同时，我们**强烈鼓励有能力和兴趣的同学挑战自我，通过网页抓取（Web Scraping）获取自己的数据集**，这将作为重要的加分项，并能让你体验真正意义上的全栈数据科学。

# 2. 核心设计理念 (Core Design Philosophy)

*   **全栈模拟 (Full-Stack Simulation):** 你将体验从数据工程到应用部署的全过程，数据获取部分可以通过使用现有数据集来模拟。
*   **跨语言协同 (Cross-Language Synergy):** 你将学习并实践现代数据科学团队的工作模式——**用最合适的工具解决最合适的问题**。我们将发挥Python在工程和AI集成上的优势，以及R在统计分析和探索性可视化上的卓越能力。
*   **AI赋能 (AI-Empowered):** 大语言模型(LLM)不再是遥远的概念，而是你手中的强大工具。你将通过API调用，将非结构化文本转化为深刻的量化洞察。
*   **产品导向 (Product-Oriented):** 你的最终目标是创造一个“产品”，而不仅仅是完成一个“作业”。这要求你不仅关注技术实现，更要思考用户体验和商业价值。

# 3. 课程目标与能力要求 (Learning Objectives)

完成本次课程设计后，你将能够：

1.  **熟练使用 Python (Pandas)** 对大型数据集进行加载和预处理。
2.  **通过编程方式调用 LLM API**，对非结构化文本数据进行高级处理与特征提取。
3.  **精通 R (`tidyverse`)** 进行复杂的数据清洗、转换和深度探索性分析。
4.  **应用 R (`ggplot2`)** 创建具有洞察力的、专业的统计图表。
5.  **在 R 中建立并解释统计模型**（如多元回归），以回答具体的业务问题。
6.  **掌握 Python (Streamlit)**，快速将数据分析结果转化为交互式Web应用。
7.  **（挑战目标）应用 Python (Playwright)** 从复杂的动态网页中采集数据。

# 4. 技术栈与推荐工具 (Technical Stack)

| 任务领域         | 核心语言 | 推荐工具/库                                             | 角色与职责                                           |
|------------------|----------|---------------------------------------------------------|------------------------------------------------------|
| **数据工程与AI** | **Python** | `Pandas`, `openai` (或其他LLM库)                          | 负责加载数据、调用AI进行智能处理，产出干净的增强数据集 |
| **统计分析核心** | **R**      | `tidyverse`, `ggplot2`, `caret`/`tidymodels`, `R Markdown` | 负责数据探索、统计建模、洞察发现，验证核心假设       |
| **应用构建与展示** | **Python** | `Streamlit`                                               | 负责将R的分析成果转化为一个可交互的Web应用           |
| **（挑战/加分）** | **Python** | `Playwright`                                              | 自行完成数据采集，替代Kaggle数据集                   |

# 5. 为期五天的冲刺流程 (Five-Day Sprint Workflow)

*   **Day 1: 蓝图设计 (Blueprint Day)**
    *   **任务:** 小组选择一个项目题目，并完成一份详细的**《项目技术方案书》**。
    *   **产出:** 方案书应包含对所选Kaggle数据集的理解、LLM分析方案、R分析思路和最终Streamlit应用的界面草图。

*   **Day 2: Python数据工程日 (Data Engineering Day)**
    *   **任务:** 用Python加载Kaggle数据集，进行初步清洗，并集中精力完成LLM API调用，生成增强数据集。
    *   **产出:** 生成高质量、已增强的`enriched_data.csv`文件。

*   **Day 3: R统计核心日 (Statistical Insight Day)**
    *   **任务:** 将接力棒交给R。进行深度的数据清洗、探索性分析、可视化和统计建模。
    *   **产出:** 一份详细的`R Markdown`过程报告，记录所有发现，并可**导出模型对象或关键分析结果**供Python使用。

*   **Day 4: Python应用开发日 (App Development Day)**
    *   **任务:** 再次回到Python。使用Streamlit读取R分析后的数据和结论，将其转化为一个美观、可交互的Web应用。
    *   **产出:** 一个本地可运行的Streamlit应用。

*   **Day 5: 产品路演与交付 (Showcase & Handoff Day)**
    *   **任务:** 进行最终的项目展示，核心是Live Demo可运行的Streamlit应用。
    *   **产出:** 提交一个组织良好的GitHub仓库，包含所有代码、数据和文档。

---

# 6. 项目选题（六选一）

请从以下六个项目中选择一个进行开发。每个项目都提供了**一个经过严格验证的、单一来源的Kaggle数据集**，并为每个阶段提供了更详细的任务建议。

### 选题一：Netflix内容策略分析系统

*   **推荐Kaggle数据集:** [**Netflix Movies and TV Shows**](https://www.kaggle.com/datasets/shivamb/netflix-shows)
    *   **数据集简介:** 包含约8800部Netflix上的影视作品。关键变量包括：`title`, `type`, `director`, `country`, `release_year`, `rating` (分级), 以及至关重要的 `description` (剧情简介)。
*   **Python 核心任务 (智能分析):**
    *   **基础任务:** 对`description`文本进行处理，提取3-5个“主题关键词”（如“悬疑”、“家庭关系”）。
    *   **进阶思路:** 判断剧情简介所传达的“情绪基调”（如“黑暗沉重”、“轻松愉快”、“紧张刺激”）。
    *   **开放探索:** 能否根据剧情简介的语义，为每部作品生成一个“推荐给喜欢XX的观众”的推荐语？
*   **R 核心任务:**
    *   **基础任务:** 使用卡方检验，分析“情绪基调”与作品分级`rating`之间是否存在显著关联。
    *   **进阶思路:** 建立逻辑回归模型，探究哪些“主题关键词”更能预测一部作品是`Movie`还是`TV Show`。
    *   **开放探索:** 使用`tidytext`包进行词频分析，找出不同国家`country`出品的影视作品在高频词上有何差异，并进行可视化。
*   **最终系统 (Python-Streamlit):**
    *   **基础功能:** 提供筛选器（按类型、国家），并以卡片形式展示作品及其AI生成的标签。
    *   **核心功能:** **可视化R的分析结果**。例如，用户选择一个“主题”，系统展示一个由R计算得出的、该主题下电影和电视剧的占比饼图。
    *   **进阶功能:** 创建一个“内容发现”页面，用户选择一种“情绪基调”，系统推荐符合该基调的作品列表。

### 选题二：Zomato餐厅成功秘诀探测器

*   **推荐Kaggle数据集:** [**Zomato Bangalore Restaurants**](https://www.kaggle.com/datasets/himanshupoddar/zomato-bangalore-restaurants)
    *   **数据集简介:** 包含班加罗尔地区超过5万家餐厅的数据。关键变量：`name`, `online_order`, `book_table`, `rate` (评分), `votes`, `location`, `rest_type`, `cuisines`, `approx_cost(for two people)`, 以及 `reviews_list` 文本字段。
*   **Python 核心任务 (智能分析):**
    *   **基础任务:** 对`reviews_list`中的评论进行处理，总结出该餐厅被频繁提及的“优点”和“缺点”标签，并计算其数量。
    *   **进阶思路:** 识别出评论中被频繁点赞的“推荐菜”和被频繁吐槽的“避雷菜”。
    *   **开放探索:** 能否根据一家餐厅所有评论的整体“气质”，为其生成一个“餐厅人设”标签（如“网红打卡地”、“家庭聚餐首选”、“深夜食堂”）？
*   **R 核心任务:**
    *   **基础任务:** 建立一个多元回归模型，预测餐厅评分`rate`。自变量包括`online_order`, `approx_cost`等基础变量。
    *   **进阶思路:** **将由AI提取的“优点数量”和“缺点数量”作为新变量加入回归模型**，分析其显著性，并对比加入前后模型的R²变化。
    *   **开放探索:** 研究`cuisines`(菜系)和`location`(地点)这两个变量的交互作用对评分的影响。是否某些菜系在特定地区更受欢迎？
*   **最终系统 (Python-Streamlit):**
    *   **基础功能:** 允许用户按菜系和地点搜索餐厅。
    *   **核心功能:** 构建一个“餐厅评分预测器”。**该应用的核心是加载R训练出的回归模型**。用户输入餐厅的各项特征（是否可预定、人均消费、AI分析出的优缺点数），系统实时输出一个预测评分。
    *   **进阶功能:** 在每个餐厅的详情页，展示AI提取的“推荐菜”和“避雷菜”。

### 选题三：酒店评论深度洞察

*   **推荐Kaggle数据集:** [**Hotel Reviews**](https://www.kaggle.com/datasets/datafiniti/hotel-reviews)
    *   **数据集简介:** 包含多家酒店的评价数据。关键变量：`name` (酒店名), `city`, `country`, `reviews.rating` (评论者打分), `reviews.title` (评论标题), `reviews.text` (评论全文)。
*   **Python 核心任务 (智能分析):**
    *   **基础任务:** 对`reviews.text`进行处理，提取旅客关注的核心要点（如“位置便利”、“早餐丰富”、“房间干净”、“隔音差”）。
    *   **进阶思路:** 识别评论的情感倾向，并将其与`reviews.rating`进行对比，找出“高分低情”或“低分高情”的矛盾评论。
    *   **开放探索:** 让LLM将一条长评论压缩成一个标准的“三段式摘要”：【优点】...【缺点】...【总结】...
*   **R 核心任务:**
    *   **基础任务:** 进行探索性数据分析，计算每个“关注点”的平均`reviews.rating`。
    *   **进阶思路:** 使用t检验或ANOVA，判断不同“关注点”对应的平均分之间是否存在统计上的显著差异。
    *   **开放探索:** 分析评论标题`reviews.title`的长度或关键词是否与最终评分`reviews.rating`相关。
*   **最终系统 (Python-Streamlit):**
    *   **基础功能:** 用户选择城市和酒店，查看基本信息和评论。
    *   **核心功能:** 开发“酒店口碑透视镜”。**该应用的核心是展示R的分析洞察**。在酒店详情页，用条形图标注出每个“关注点”对应的平均分（数据由R计算），让用户一目了然地看到酒店的强项和弱项。
    *   **进阶功能:** 提供一个“智能摘要”按钮，点击后显示由AI生成的“三段式摘要”。

### 选题四：Ames房价精准评估器

*   **推荐Kaggle数据集:** [**Ames Housing Dataset**](https://www.kaggle.com/datasets/prevek18/ames-housing-dataset)
    *   **数据集简介:** 这是机器学习领域经典的房价预测数据集，包含爱荷华州Ames市的近3000条房产记录和79个解释变量，如`GrLivArea`(地上居住面积), `OverallQual`(总体质量), `YearBuilt`(建造年份), `Neighborhood`(街区)等。它缺少长文本，但非常适合进行特征工程和生成式AI应用。
*   **Python 核心任务 (智能分析):**
    *   **基础任务:** 由于缺少描述文本，此处的AI任务是**生成式**的。让LLM根据房子的结构化数据（如`OverallQual`, `GrLivArea`, `BedroomAbvGr`等），为每条记录生成一段吸引人的`description`文本。
    *   **进阶思路:** 让LLM根据`Neighborhood`的名称和房产数据，为每个街区生成一个“社区画像”标签（如“宁静的郊区”、“繁华的市中心”、“学者社区”）。
    *   **开放探索:** 能否让LLM根据所有特征，为房子生成一个最有可能的“潜在买家画像”（如“首次购房的年轻夫妇”、“需要大院子的家庭”）？
*   **R 核心任务:**
    *   **基础任务:** 建立一个预测`SalePrice`(售价)的多元回归模型，使用`GrLivArea`, `OverallQual`等数值和类别型变量。
    *   **进阶思路:** 进行特征工程，例如创建`HouseAge` (`YrSold` - `YearBuilt`)变量。将AI生成的“社区画像”作为新变量加入模型，看其是否能提升模型解释力。
    *   **开放探索:** 使用随机森林或GBM等更复杂的模型进行预测，并通过变量重要性分析找出影响房价的最关键因素。
*   **最终系统 (Python-Streamlit):**
    *   **基础功能:** 展示Ames市的房产数据分布图。
    *   **核心功能:** 创建一个“房价评估器”。**该应用的核心是加载R训练出的预测模型**。用户通过滑块和下拉菜单输入房子的各项参数，系统会利用R模型实时给出一个预测售价。
    *   **进阶功能:** 在预测结果旁边，展示由AI为该虚拟房产生成的`description`描述文本。

### 选题五：二手车交易洞察平台

*   **推荐Kaggle数据集:** [**Craigslist Used Cars Dataset**](https://www.kaggle.com/datasets/austinreese/craigslist-carstrucks-data)
    *   **数据集简介:** 包含美国Craigslist上近42.7万条二手车挂牌信息。关键变量：`price`, `year`, `manufacturer`, `model`, `condition`, `odometer`(里程), `transmission`, 以及至关重要的 `description`(卖家描述)。
*   **Python 核心任务 (智能分析):**
    *   **基础任务:** 对`description`文本进行处理，提取车辆的额外特征（如“一手车主”、“无事故”、“新轮胎”）。
    *   **进阶思路:** 识别卖家描述中的“销售话术”或“风险词汇”（如“as-is/现状出售”、“salvage title/报废车辆翻新”）。
    *   **开放探索:** 让LLM根据描述文本，评估卖家的“可靠度”或“诚意度”（例如，描述详细、说明缺点的可能更可靠）。
*   **R 核心任务:**
    *   **基础任务:** 建立一个预测`price`的多元回归模型，自变量包括`year`, `odometer`等。
    *   **进阶思路:** **将AI提取的“额外特征”和“风险词汇”作为新变量加入模型**，分析它们对价格的显著影响。例如，“无事故”标签是否能显著提升售价？
    *   **开放探索:** 构建一个“交易划算度”指标（如 `deal_score = model_predicted_price - listing_price`），找出市场上可能被低估的“捡漏”车源。
*   **最终系统 (Python-Streamlit):**
    *   **基础功能:** 搜索和筛选二手车信息。
    *   **核心功能:** 构建“二手车估价与风险检测器”。**应用的核心是加载R的模型和洞察**。用户输入车辆信息，系统给出估价。同时，当用户粘贴一段卖家描述时，系统能高亮显示AI识别出的“风险词汇”。
    *   **进阶功能:** 创建一个“今日好deal”页面，展示R分析出的“交易划算度”最高的Top 10车源。

### 选题六：数据分析师就业市场导航

*   **推荐Kaggle数据集:** [**Data Analyst Jobs**](https://www.kaggle.com/datasets/andrewmvd/data-analyst-jobs)
    *   **数据集简介:** 包含超过2000个数据分析师的招聘信息。关键变量：`Job Title`, `Salary Estimate`, `Job Description`, `Rating` (公司评分), `Company Name`, `Location`, `Industry`。
*   **Python 核心任务 (智能分析):**
    *   **基础任务:** 对`Job Description`文本进行处理，提取并归一化所需的硬技能（如`Python`, `R`, `SQL`, `Tableau`）和软技能（如`communication`）。
    *   **进阶思路:** 推断职位的 seniority level (Junior, Senior, Lead)，并识别出职位描述中是否包含管理职责。
    *   **开放探索:** 能否让AI根据JD，为求职者生成一段针对该职位的“求职信草稿”或“面试准备要点”？
*   **R 核心任务:**
    *   **基础任务:** 将`Salary Estimate`处理为数值（如取中位数）。建立一个薪资预测的多元回归模型。
    *   **进阶思路:** **核心是分析“技能溢价”**。在模型中，探究每项硬技能和软技能对薪资的影响力（即模型的系数）。例如，掌握`Python`比掌握`Excel`能带来多高的薪资溢价？
    *   **开放探索:** 使用关联规则挖掘（Apriori算法），找出最常同时出现的技能组合（如`SQL`和`Tableau`总是被一起要求）。
*   **最终系统 (Python-Streamlit):**
    *   **基础功能:** 展示不同行业的薪资分布。
    *   **核心功能:** 创建“职业技能价值评估器”。**该应用的核心是加载R训练出的模型系数**。用户勾选自己掌握的技能，系统会根据R模型计算出的各技能“价值”，累加得出一个预估的薪资范围。
    *   **进阶功能:** 创建一个“技能组合推荐”页面，展示R分析出的最热门的技能组合，帮助学生规划学习路径。

---

# 7. 最终交付成果 (Final Deliverables)

你需要提交一个**单一的GitHub项目仓库链接**，该仓库应包含以下内容，并有清晰的目录结构：

1.  **`README.md`:** 项目的“门面”。详细说明项目背景、功能、如何运行你的Streamlit应用，以及小组成员分工。（如果完成了挑战，请在此特别说明）
2.  **`/src` 目录:**
    *   `data_processing.py`: Python脚本，负责加载数据和调用LLM API。
    *   `analysis.R` 或 `analysis.Rmd`: R脚本/文档，包含所有数据分析和建模代码。
    *   `app.py`: 最终的Streamlit应用代码。
    *   `（挑战）scraper.py`: 如果你完成了挑战，请将爬虫脚本放在这里。
3.  **`/data` 目录:**
    *   `enriched_data.csv`: 经过LLM处理后的、供R分析的完整数据集。
    *   `（可选）model.rds`: 由R保存的模型对象，供Python加载。
4.  **`/reports` 目录:**
    *   `Project_Proposal.pdf`: 第一天完成的技术方案书。
    *   `R_Analysis_Report.html`: 由`analysis.Rmd`生成的HTML报告，展示分析过程与发现。
5.  **`requirements.txt`:** 运行Python脚本所需的所有库。
6.  **`AI_Collaboration_Log.md`:** (可选但强烈推荐) 记录你们在项目中如何与AI（ChatGPT, Copilot等）协作解决问题的日志。

# 8. 评估标准 (Assessment Criteria)

| 评估维度                           | 权重  | 核心考察点                                                                                             |
|------------------------------------|-------|--------------------------------------------------------------------------------------------------------|
| **数据工程与AI应用 (Python)**        | 30%   | 数据加载与处理的质量；**LLM API调用的创新性与有效性**；代码的规范性与效率。                      |
| **统计分析与建模深度 (R)**         | 30%   | EDA的洞察力；统计方法与模型的恰当性；`ggplot2`可视化的专业性；对模型结果的解释能力。         |
| **最终应用与产品价值 (Python)**      | 25%   | Streamlit应用的交互性、稳定性和用户体验；**能否清晰、有效地体现R的分析结论**；功能的完整性与创新性。 |
| **项目管理与文档规范**             | 15%   | GitHub仓库的组织清晰度；`README`文档的质量；代码注释与可读性；团队协作与项目进度管理。         |
| **挑战加分项**                     | 最高+10% | **成功使用Playwright获取自定义、高质量数据的团队，将在总分基础上获得额外加分。**                 |

:::{.callout-note}
## 关于API Key
本项目需要使用LLM API，这通常需要个人申请并可能涉及少量费用。请提前准备好你的API Key。在代码中，**严禁硬编码你的API Key**，应使用环境变量或配置文件等安全方式进行管理。
:::

:::{.callout-warning}
## 学术诚信
所有代码和分析必须由小组成员独立完成。允许使用AI工具辅助编程和解决问题，但必须在日志中记录。严禁任何形式的抄袭。一经发现，本次课程设计成绩将记为零分。
:::